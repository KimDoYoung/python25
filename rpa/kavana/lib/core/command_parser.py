from datetime import date, datetime
import re
import os
from typing import Any, Dict, List, Tuple, Union
from lib.core.command_preprocessor import CommandPreprocessor, PreprocessedLine
from lib.core.datatypes.hash_map import HashMap
from lib.core.datatypes.kavana_datatype import Boolean,  Float, Integer, KavanaDataType, NoneType, String
from lib.core.datatypes.array import Array
from lib.core.datatypes.point import Point
from lib.core.datatypes.ymd_time import YmdTime
from lib.core.exception_registry import ExceptionRegistry
from lib.core.exceptions.kavana_exception import CommandParserError, DataTypeError, KavanaSyntaxError
from lib.core.token import ArrayToken, AccessIndexToken, HashMapToken, StringToken,  Token
from lib.core.token_type import TokenType
from lib.core.function_registry import FunctionRegistry
from lib.core.token_util import TokenUtil

class CommandParser:
    """
    Kavana ìŠ¤í¬ë¦½íŠ¸ì˜ ëª…ë ¹ì–´ë¥¼ ë¶„ì„í•˜ëŠ” íŒŒì„œ.
    - `main ... end_main` ë¸”ë¡ ì•ˆì—ì„œ ì‹¤í–‰
    - `INCLUDE` â†’ ì™¸ë¶€ KVS íŒŒì¼ í¬í•¨
    - `ENV_LOAD` â†’ .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ `SET`ìœ¼ë¡œ ë³€í™˜
    """
    def __init__(self, script_lines=[], base_path="."):
        self.script_lines = script_lines
        self.base_path = base_path  # ìŠ¤í¬ë¦½íŠ¸ ê¸°ë³¸ ê²½ë¡œ (INCLUDE, LOAD ì²˜ë¦¬ìš©)
        self.in_main_block = False

    def parse(self, lines = []):
        """
        âœ… ìŠ¤í¬ë¦½íŠ¸ì˜ ëª¨ë“  ëª…ë ¹ì–´ë¥¼ ë¶„ì„í•˜ì—¬ `Token` ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
        """
        self.in_main_block = False
        parsed_commands = []
        if lines:
            self.script_lines = lines
        processed_lines = self.script_lines
        i = 0  

        while i < len(processed_lines):
            tokens = self.tokenize(processed_lines[i])  # âœ… `Token` ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            
            if not tokens:
                i += 1
                continue

            cmd = tokens[0].data.value.upper()  # âœ… ëª…ë ¹ì–´ëŠ” ëŒ€ë¬¸ìë¡œ ë³€í™˜
            args = tokens[1:]  # âœ… ë‚˜ë¨¸ì§€ëŠ” `Token` ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ìœ ì§€

            # âœ… ë¸”ë¡ ëª…ë ¹ì–´ ì²˜ë¦¬
            if cmd in ["IF", "WHILE", "FOR"]:
                end_mapping = {"IF": "END_IF", "WHILE": "END_WHILE", "FOR": "END_FOR"}
                block_body, new_index = self.parse_block(processed_lines, i + 1, end_mapping[cmd])
                parsed_commands.append({"cmd": f"{cmd}_BLOCK", "body": [{"cmd": cmd, "args": args}] + block_body})
                i = new_index + 1
                continue

            # âœ… FUNCTION ì²˜ë¦¬
            if cmd == "FUNCTION":
                i = self.parse_function(processed_lines, i)
                continue  # âœ… í•¨ìˆ˜ ì •ì˜ëŠ” parsed_commandsì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ

            if cmd == "ON_EXCEPTION":
                i = self.parse_exception(processed_lines, i)
                continue  # âœ… í•¨ìˆ˜ ì •ì˜ëŠ” parsed_commandsì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ

            # âœ… INCLUDE ì²˜ë¦¬
            if cmd == "INCLUDE":
                if not args:
                    raise SyntaxError("INCLUDE ë¬¸ì— íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                include_path = args[0].data.value.strip('"')  # âœ… Token ê°ì²´ì—ì„œ ê°’ ì¶”ì¶œ
                self._include_process(include_path, parsed_commands)
                i += 1
                continue

            # âœ… LOAD ì²˜ë¦¬
            if cmd == "ENV_LOAD":
                if not args:
                    raise SyntaxError("ENV_LOAD ë¬¸ì— .env íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                env_path = args[0].data.value.strip('"')  # âœ… Token ê°ì²´ì—ì„œ ê°’ ì¶”ì¶œ
                self._env_load(env_path, parsed_commands)
                i += 1
                continue

            # âœ… MAIN ë¸”ë¡ ì²˜ë¦¬
            if cmd == "MAIN":
                if not getattr(self, "ignore_main_check", False):
                    if self.in_main_block:
                        raise CommandParserError("ì¤‘ë³µëœ MAINë¬¸ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ", line = i+1)
                    self.in_main_block = True
                i += 1
                continue
            # âœ… END_MAIN ì²˜ë¦¬
            if cmd == "END_MAIN":
                if not getattr(self, "ignore_main_check", False):
                    if not self.in_main_block:
                        raise CommandParserError("'END_MAIN'ì´ 'MAIN' ë¬¸ ì—†ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.", line = i+1)
                    self.in_main_block = False
                i += 1
                break

            # âœ… MAIN ë¸”ë¡ ì™¸ë¶€ì—ì„œ ëª…ë ¹ì–´ ì‚¬ìš© ì œí•œ
            if not self.in_main_block and not getattr(self, "ignore_main_check", False):
                raise CommandParserError("ëª…ë ì–´ëŠ” MAIN ë¸”ë¡ ë‚´ì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.", line = i+1)

            # âœ… ì¼ë°˜ ëª…ë ¹ì–´ ì¶”ê°€
            parsed_commands.append({"cmd": cmd, "args": args})  # âœ… `args`ë„ `Token` ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
            i += 1  

        if self.in_main_block:
            raise CommandParserError("'END_MAIN'ë¬¸ì´ ë¹ ì¡ŒìŠµë‹ˆë‹¤.", line = i+1)

        return parsed_commands

    def parse_block(self, ppLines: List[PreprocessedLine], start_line, end_keyword):
        """ì¬ê·€ì ìœ¼ë¡œ ë¸”ë¡ì„ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜"""
        block_body = []
        i = start_line

        while i < len(ppLines):
            line = ppLines[i].text.strip().upper()

            if line == end_keyword:
                return block_body, i  # âœ… END í‚¤ì›Œë“œë¥¼ ë§Œë‚˜ë©´ ì¢…ë£Œ

            tokens = self.tokenize(ppLines[i])
            if not tokens:
                i += 1
                continue

            cmd = tokens[0].data.value.upper()
            args = tokens[1:]

            # âœ… ì¤‘ì²©ëœ ë¸”ë¡ ì²˜ë¦¬ (IF, WHILE, FOR)
            if cmd in ["IF", "WHILE", "FOR"]:
                end_mapping = {"IF": "END_IF", "WHILE": "END_WHILE", "FOR": "END_FOR"}
                nested_block, new_index = self.parse_block(ppLines, i + 1, end_mapping[cmd])
                block_body.append({"cmd": f"{cmd}_BLOCK", "body": [{"cmd": cmd, "args": args}] + nested_block})
                i = new_index + 1
                continue

            # âœ… ì¼ë°˜ ëª…ë ¹ì–´ ì¶”ê°€
            block_body.append({"cmd": cmd, "args": args})
            i += 1

        raise CommandParserError(f"{end_keyword}ê°€ ì—†ìŠµë‹ˆë‹¤.")  # âœ… ì¢…ë£Œ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°œìƒ

    def parse_function(self, ppLines:List[PreprocessedLine], start_line):
        """FUNCTION ë¸”ë¡ì„ íŒŒì‹±í•˜ì—¬  headerë¥¼ ì œì™¸í•˜ê³  FunctionRegistryì— ì €ì¥"""
        func_def_lines = [ppLines[start_line]]
        i = start_line + 1

        while i < len(ppLines) and ppLines[i].text.strip().upper() != "END_FUNCTION":
            func_def_lines.append(ppLines[i])
            i += 1

        if i >= len(ppLines) or ppLines[i].text.strip().upper() != "END_FUNCTION":
            raise CommandParserError("í•¨ìˆ˜ ì •ì˜ì—ì„œ END_FUNCTIONì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.", start_line, 0)
        i += 1  # âœ… END_FUNCTION ìŠ¤í‚µ

        # âœ… í•¨ìˆ˜ í—¤ë” íŒŒì‹±
        header_tokens = self.tokenize(func_def_lines[0])
        if len(header_tokens) < 2:
            raise CommandParserError("í•¨ìˆ˜ ì •ì˜ í—¤ë”ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.", start_line, 0)

        func_name = header_tokens[1].data.value
        params = []
        
        if len(header_tokens) > 2 and header_tokens[2].data.value == "(":
            # index 2ë¶€í„° header_tokensì˜ ëê¹Œì§€ë¥¼ ê°€ì ¸ì™€ì„œ ")"ë¥¼ ì œê±° function plus a b 
            param_str = " ".join([t.data.value for t in header_tokens[3:]]).rstrip(")")
            params = [p.strip() for p in param_str.split(",")]
        elif len(header_tokens) > 2:
            params = header_tokens[2:]

        # âœ… `CommandParser`ë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ìˆ˜ ë³¸ë¬¸ì„ ë¯¸ë¦¬ íŒŒì‹±í•˜ì—¬ ì €ì¥
        parser = CommandParser()
        parser.ignore_main_check = True  # âœ… MAIN ë¸”ë¡ ê²€ì‚¬ ë¬´ì‹œ
        parsed_commands = parser.parse(func_def_lines[1:])

        # âœ… FunctionRegistryì— ì €ì¥
        FunctionRegistry.register_function(func_name, params, parsed_commands)
        return i  # âœ… í•¨ìˆ˜ ì •ì˜ í›„ ìƒˆë¡œìš´ ë¼ì¸ ë²ˆí˜¸ ë°˜í™˜

    def parse_exception(self, ppLines: List[PreprocessedLine], start_line: int):
        """ON_EXCEPTION ë¸”ë¡ì„ íŒŒì‹±í•˜ì—¬ headerë¥¼ ì œì™¸í•˜ê³  FunctionRegistryì— ì €ì¥"""
        exception_def_lines = [ppLines[start_line]]  # âœ… ON_EXCEPTION í¬í•¨
        i = start_line + 1

        while i < len(ppLines) and ppLines[i].text.strip().upper() != "END_EXCEPTION":
            exception_def_lines.append(ppLines[i])
            i += 1

        if i >= len(ppLines):
            raise CommandParserError("ì˜ˆì™¸ ì²˜ë¦¬ ì •ì˜ì—ì„œ END_EXCEPTIONì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.", start_line, 0)

        # âœ… END_EXCEPTIONë„ ì˜ˆì™¸ ì²˜ë¦¬ ë¸”ë¡ì— í¬í•¨
        exception_def_lines.append(ppLines[i])  
        i += 1
        
        # âœ… ìˆ˜ì •ëœ ë¸”ë¡ì„ ë“±ë¡
        ExceptionRegistry.register_exception(exception_def_lines)
        
        return i  # ë‹¤ìŒ ë¼ì¸ ì¸ë±ìŠ¤ ë°˜í™˜

    def _include_process(self, include_path, parsed_commands):
        """INCLUDE ë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ ì™¸ë¶€ KVS íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¨ë‹¤."""
        """âœ… `INCLUDE "íŒŒì¼.kvs"` ì²˜ë¦¬ (ìƒëŒ€ ê²½ë¡œ ì§€ì›)"""
        file_path = os.path.join(self.base_path, include_path)  # âœ… ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ ë³€í™˜
        full_path = os.path.abspath(file_path)  # âœ… ìµœì¢… ì ˆëŒ€ ê²½ë¡œ ë³€í™˜

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"INCLUDE íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path}")

        with open(full_path, "r", encoding="utf-8") as file:
            included_lines = file.readlines()

        # âœ… ê¸°ì¡´ ìƒíƒœ ì €ì¥
        original_ignore_main_check = getattr(self, "ignore_main_check", False)

        # âœ… INCLUDE ì‹¤í–‰ ì¤‘ì—ëŠ” MAIN ë¸”ë¡ ì—¬ë¶€ë¥¼ ë¬´ì‹œí•˜ë„ë¡ ì„¤ì •
        self.ignore_main_check = True
        preprocessor = CommandPreprocessor()
        ppLines = preprocessor.preprocess(included_lines)

        included_parser = CommandParser(ppLines, self.base_path)
        included_parser.ignore_main_check = True  # âœ… ì„œë¸Œ íŒŒì„œì—ì„œë„ MAIN ë¸”ë¡ ê²€ì‚¬ ë¬´ì‹œ

        included_commands = included_parser.parse()

        # âœ… INCLUDE ì™„ë£Œ í›„ ì›ë˜ ìƒíƒœ ë³µì›
        self.ignore_main_check = original_ignore_main_check

        parsed_commands.extend(included_commands)  # INCLUDEëœ ëª…ë ¹ì–´ ì¶”ê°€

    def _env_load(self, env_path, parsed_commands):
    
        # âœ… ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        full_path = os.path.abspath(os.path.join(self.base_path, env_path))

        if not os.path.exists(full_path):
            raise FileNotFoundError(f"ENV_LOAD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")

        with open(full_path, "r", encoding="utf-8") as file:
            for line in file:
                line = line.strip()
                if not line or line.startswith("#"):  # âœ… ì£¼ì„ ë° ë¹ˆ ì¤„ ë¬´ì‹œ
                    continue

                if "=" not in line:
                    raise CommandParserError(f"ì˜ëª»ëœ í™˜ê²½ ë³€ìˆ˜ í˜•ì‹ env line : {line}")

                key, value = line.split("=", 1)
                key = key.strip().upper()  # âœ… `$KEY` í˜•íƒœë¡œ ì €ì¥
                value = value.strip()

                # âœ… ê°’ì˜ íƒ€ì…ì„ íŒë³„í•˜ì—¬ Tokenìœ¼ë¡œ ë³€í™˜
                if value.lower() in ["true", "false"]:
                    value_token = Token(data=Boolean(value.lower() == "true"), type=TokenType.BOOLEAN)
                elif value.isdigit():
                    value_token = Token(data=Integer(int(value)), type=TokenType.INTEGER)
                elif value.replace(".", "", 1).isdigit():  # âœ… Float íŒë³„
                    value_token = Token(data=Float(float(value), type=TokenType.FLOAT))
                elif  (value.startswith('"') and value.endswith('"')) or (value.startswith("'") and value.endswith("'")):
                    value =value[1:-1]  # ì•ë’¤ ë”°ì˜´í‘œ ì œê±°
                    value_token = Token(data=String(value), type=TokenType.STRING)
                else:
                    value_token = Token(data=String(value), type=TokenType.STRING)

                # âœ… "=" ì—°ì‚°ì í† í° ì¶”ê°€
                equals_token = Token(data=String("="), type=TokenType.ASSIGN)

                key_token = Token(data=String(f"${key}"), type=TokenType.IDENTIFIER)
                # âœ… `parsed_commands`ì— ì¶”ê°€í•˜ì—¬ ì¶”ì  ê°€ëŠ¥
                parsed_commands.append({
                    "cmd": "CONST",
                    "args": [key_token, equals_token, value_token]
                })

    @staticmethod
    def decode_escaped_string(s: str) -> str:
        ESCAPE_MAP = {
            "n": "\n",
            "t": "\t",
            "r": "\r",
            "b": "\b",
            "f": "\f",
            "v": "\v",
            "\\": "\\",
            '"': '"',
        }        
        result = []
        i = 0
        while i < len(s):
            if s[i] == "\\":
                if i + 1 >= len(s):
                    raise ValueError("ì˜ëª»ëœ ë¬¸ìì—´: ë‹¨ë… ë°±ìŠ¬ë˜ì‹œ(`\\`)ê°€ í¬í•¨ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                escape_seq = s[i + 1]
                result.append(ESCAPE_MAP.get(escape_seq, "\\" + escape_seq))
                i += 2
            else:
                result.append(s[i])
                i += 1
        return "".join(result)


    # @staticmethod
    # def decode_escaped_string(s: str) -> str:
    #     """âœ… C ìŠ¤íƒ€ì¼ escape ë³€í™˜ (`"\\n"` â†’ `"\n"`)"""
    #     result = []
    #     i = 0
    #     while i < len(s):
    #         if s[i] == "\\":
    #             if i + 1 >= len(s):  # ğŸ”¥ ë‹¨ë… ë°±ìŠ¬ë˜ì‹œëŠ” ì˜¤ë¥˜
    #                 raise ValueError("ì˜ëª»ëœ ë¬¸ìì—´: ë‹¨ë… ë°±ìŠ¬ë˜ì‹œ(`\\`)ê°€ í¬í•¨ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    #             escape_seq = s[i + 1]

    #             if escape_seq == "n":
    #                 result.append("\n")  # âœ… `\\n` â†’ `\n` (ê°œí–‰ ë¬¸ì ë³€í™˜)
    #             elif escape_seq == "t":
    #                 result.append("\t")  # âœ… `\\t` â†’ `\t` (íƒ­ ë¬¸ì ë³€í™˜)
    #             elif escape_seq == "\\":
    #                 result.append("\\")  # âœ… `\\` â†’ `\`
    #             elif escape_seq == '"':
    #                 result.append('"')  # âœ… `\"` â†’ `"`
    #             else:
    #                 result.append("\\" + escape_seq)  # âœ… ì•Œ ìˆ˜ ì—†ëŠ” escape ë¬¸ì ìœ ì§€

    #             i += 2  # ğŸ”¥ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ë‘ ê¸€ì ê±´ë„ˆë›°ê¸°
    #         else:
    #             result.append(s[i])
    #             i += 1

    #     return "".join(result)

    # def decode_escaped_string(s: str) -> str:
    #     """âœ… 1ë°”ì´íŠ¸ì”© ì½ì–´ê°€ë©´ì„œ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ë³€í™˜"""
    #     result = []
    #     i = 0
    #     while i < len(s):
    #         if s[i] == "\\" and i + 1 < len(s):  # ğŸ”¥ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ë°œê²¬
    #             escape_seq = s[i + 1]

    #             if escape_seq == "n":
    #                 result.append("\n")
    #             elif escape_seq == "t":
    #                 result.append("\t")
    #             elif escape_seq == "\\":
    #                 result.append("\\")
    #             elif escape_seq == '"':
    #                 result.append('"')
    #             else:
    #                 result.append("\\" + escape_seq)  # âœ… ë¯¸ë¦¬ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶”ê°€

    #             i += 2  # ğŸ”¥ ì´ìŠ¤ì¼€ì´í”„ ë¬¸ìëŠ” 2ë°”ì´íŠ¸ ì²˜ë¦¬
    #         else:
    #             result.append(s[i])
    #             i += 1

    #     return "".join(result)
    
    # @staticmethod
    # def tokenize(ppLine: PreprocessedLine) -> list:
    #     """í•œ ì¤„ì„ `Token` ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""

    #     line = ppLine.text.strip()
    #     tokens = []

    #     token_patterns = [

    #         # âœ… ë…¼ë¦¬ ê°’
    #         (r'\bTrue\b', TokenType.BOOLEAN),
    #         (r'\bFalse\b', TokenType.BOOLEAN),
    #         (r'\bNone\b', TokenType.NONE),
    #         # âœ… ë°ì´í„° íƒ€ì… í‚¤ì›Œë“œ
    #         (r'(?i)\bPOINT\b', TokenType.POINT),
    #         (r'(?i)\bREGION\b', TokenType.REGION),
    #         (r'(?i)\bRECTANGLE\b', TokenType.RECTANGLE),
    #         (r'(?i)\bIMAGE\b', TokenType.IMAGE),
    #         (r'(?i)\bWINDOW\b', TokenType.WINDOW),  
    #         (r'(?i)\bAPPLICATION\b', TokenType.APPLICATION),            
            
    #         (r'(?i)\bGLOBAL\b', TokenType.GLOBAL),

    #         # âœ… ì œì–´ë¬¸ í‚¤ì›Œë“œ
    #         (r'(?i)\bEND_FUNCTION\b', TokenType.END_FUNCTION),
    #         (r'(?i)\bEND_WHILE\b', TokenType.END_WHILE),
    #         (r'(?i)\bFUNCTION\b', TokenType.FUNCTION),
    #         # âœ… ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê´€ë ¨ í‚¤ì›Œë“œ
    #         (r'(?i)\bENV_LOAD\b', TokenType.ENV_LOAD),
    #         (r'(?i)\bEND_MAIN\b', TokenType.END_MAIN),
    #         (r'(?i)\bINCLUDE\b', TokenType.INCLUDE),
    #         (r'(?i)\bRETURN\b', TokenType.RETURN),
    #         (r'(?i)\bMAIN\b', TokenType.MAIN),
    #         (r'(?i)\bEND_FOR\b', TokenType.END_FOR),
    #         (r'(?i)\bEND_IF\b', TokenType.END_IF),
    #         (r'(?i)\bWHILE\b', TokenType.WHILE),
    #         (r'(?i)\bSTEP\b', TokenType.STEP), 
    #         (r'(?i)\bELSE\b', TokenType.ELSE),
    #         (r'(?i)\bELIF\b', TokenType.ELIF),
    #         (r'(?i)\bIF\b', TokenType.IF),
    #         (r'(?i)\bFOR\b', TokenType.FOR),
    #         (r'(?i)\bTO\b', TokenType.TO),  
    #         (r'(?i)\bIN\b', TokenType.IN), 

    #         # âœ… í•¨ìˆ˜ ê´€ë ¨ í‚¤ì›Œë“œ

    #         # âœ… ë…¼ë¦¬ ì—°ì‚°ì
    #         (r'(?i)\bAND\b', TokenType.LOGICAL_OPERATOR), 
    #         (r'(?i)\bOR\b', TokenType.LOGICAL_OPERATOR), 
    #         (r'(?i)\bNOT\b', TokenType.LOGICAL_OPERATOR),

    #         # âœ… ë£¨í”„ ì œì–´ í‚¤ì›Œë“œ
    #         (r'(?i)\bBREAK\b', TokenType.BREAK),
    #         (r'(?i)\bCONTINUE\b', TokenType.CONTINUE),
    #         # âœ… YmdTime íŒ¨í„´ ì¶”ê°€ (ê´„í˜¸ í•„ìˆ˜)
    #         (r"(?i)\bYmdTime\b", TokenType.IDENTIFIER),
    #         (r"(?i)\bYmd\b", TokenType.IDENTIFIER),

    #         # âœ… ì‘ì€ë”°ì˜´í‘œ ì‚¬ìš© ê°ì§€ (ë¬¸ë²• ì˜¤ë¥˜ ì²˜ë¦¬)
    #         (r"'([^']*)'", None),  # âŒ ì‘ì€ë”°ì˜´í‘œê°€ ê°ì§€ë˜ë©´ ì˜ˆì™¸ ë°œìƒ

    #         # âœ… ì—°ì‚°ì
    #         (r'\(', TokenType.LEFT_PAREN),
    #         (r'\)', TokenType.RIGHT_PAREN),
    #         (r'\[', TokenType.LEFT_BRACKET),
    #         (r'\]', TokenType.RIGHT_BRACKET),
    #         (r'\{', TokenType.LEFT_BRACE),
    #         (r'\}', TokenType.RIGHT_BRACE),
    #         (r',', TokenType.COMMA),
    #         (r':', TokenType.COLON),


    #         (r'==|!=|>=|<=|[+\-*/%<>]', TokenType.OPERATOR),  # âœ… '=' ì œê±°
    #         (r'=', TokenType.ASSIGN),  # âœ… '='ì„ ë³„ë„ë¡œ í• ë‹¹ ì—°ì‚°ìë¡œ ë¶„ë¦¬            

    #         # âœ… ì¼ë°˜ ì‹ë³„ì  
    #         (r'[a-zA-Z_\$][a-zA-Z0-9_]*', TokenType.IDENTIFIER),

    #         # âœ… float, integer
    #         (r'\b\d+\.\d+|\.\d+|\d+\.\b', TokenType.FLOAT),  # ğŸ”¥ ì†Œìˆ˜ì ë§Œ ìˆëŠ” ê²½ìš°ë„ í¬í•¨
    #         (r'\b\d+\b', TokenType.INTEGER),         # ì •ìˆ˜ (ì˜ˆ: 10, 42, 1000)

    #         # âœ… ëª¨ë“  ìœ ë‹ˆì½”ë“œ ë¬¸ì í¬í•¨          
    #         (r'"((?:\\.|[^"\\])*)"', TokenType.STRING),  # âœ… ë¬¸ìì—´ ì •ê·œì‹ ìˆ˜ì •

    #     ]
    #     column_num = ppLine.original_column
    #     line_num = ppLine.original_line

    #     while line:
    #         matched = False

    #         # ğŸ”¥ ê³µë°±ì„ ê±´ë„ˆë›°ê³  column ì¡°ì •
    #         while line and line[0] == " ":
    #             column_num += 1
    #             line = line[1:]

    #         for pattern, token_type in token_patterns:
    #             match = re.match(pattern, line)
    #             if match:
    #                 raw_value = match.group(1) if token_type == TokenType.STRING else match.group(0)

    #                 # âŒ ì‘ì€ë”°ì˜´í‘œ(`' '`) ì‚¬ìš© ê°ì§€ ì‹œ `SyntaxError` ë°œìƒ
    #                 if token_type is None:
    #                     raise KavanaSyntaxError(
    #                         f"ì˜ëª»ëœ ë¬¸ìì—´ í˜•ì‹ì…ë‹ˆë‹¤: ìŒë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•´ ì£¼ì‹­ì‹œì˜¤ (\") ì¤„ë²ˆí˜¸ {line_num}, ì»¬ëŸ¼ë²ˆí˜¸ {column_num}"
    #                     )
    #                 if token_type == TokenType.RAW_STRING:
    #                     value = raw_value
    #                     tokens.append(Token(data=value, type=TokenType.STRING, line=line_num, column=column_num))                        
    #                 elif token_type == TokenType.STRING:
    #                     value = CommandParser.decode_escaped_string(raw_value)  # âœ… ì§ì ‘ ë³€í™˜ í•¨ìˆ˜ í˜¸ì¶œ
    #                     value_datatype_changed = TokenUtil.primitive_to_kavana_by_tokentype(value, token_type)
    #                     tokens.append(Token(data=value_datatype_changed, type=token_type, line=line_num, column=column_num))
    #                 else:
    #                     value = raw_value
    #                     value_datatype_changed = TokenUtil.primitive_to_kavana_by_tokentype(value, token_type)
    #                     tokens.append(Token(data=value_datatype_changed, type=token_type, line=line_num, column=column_num))

    #                 column_num += len(match.group(0))
    #                 line = line[len(match.group(0)):]  # âœ… `line`ì„ ì˜¬ë°”ë¥´ê²Œ ì¤„ì„

    #                 matched = True
    #                 break

    #         if not matched and line:  # âœ… ë” ì´ìƒ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ë¬¸ìê°€ ìˆìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
    #             line = line[1:]  # âœ… í•œ ê¸€ì ì¤„ì—¬ì„œ ì§„í–‰í•˜ì—¬ ë¬´í•œ ë£¨í”„ ë°©ì§€
    #             column_num += 1
    #     tokens = CommandParser.post_process_tokens(tokens)
    #     return tokens

    @staticmethod
    def tokenize(ppLine: PreprocessedLine) -> list:
        """í•œ ì¤„ì„ `Token` ë˜ëŠ” `StringToken` ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""

        import re

        line = ppLine.text.strip()
        tokens = []

        string_pattern = r'(?i)(r?f?|fr?)("((?:\\.|[^"\\])*)")'  # ì ‘ë‘ì–´ í¬í•¨ ë¬¸ìì—´

        token_patterns = [

            # âœ… ë…¼ë¦¬ ê°’
            (r'\bTrue\b', TokenType.BOOLEAN),
            (r'\bFalse\b', TokenType.BOOLEAN),
            (r'\bNone\b', TokenType.NONE),

            # âœ… ë°ì´í„° íƒ€ì… í‚¤ì›Œë“œ
            (r'(?i)\bPOINT\b', TokenType.POINT),
            (r'(?i)\bREGION\b', TokenType.REGION),
            (r'(?i)\bRECTANGLE\b', TokenType.RECTANGLE),
            (r'(?i)\bIMAGE\b', TokenType.IMAGE),
            (r'(?i)\bWINDOW\b', TokenType.WINDOW),  
            (r'(?i)\bAPPLICATION\b', TokenType.APPLICATION),
            (r'(?i)\bGLOBAL\b', TokenType.GLOBAL),

            # âœ… ì œì–´ë¬¸ í‚¤ì›Œë“œ
            (r'(?i)\bEND_FUNCTION\b', TokenType.END_FUNCTION),
            (r'(?i)\bEND_WHILE\b', TokenType.END_WHILE),
            (r'(?i)\bFUNCTION\b', TokenType.FUNCTION),
            (r'(?i)\bENV_LOAD\b', TokenType.ENV_LOAD),
            (r'(?i)\bEND_MAIN\b', TokenType.END_MAIN),
            (r'(?i)\bINCLUDE\b', TokenType.INCLUDE),
            (r'(?i)\bRETURN\b', TokenType.RETURN),
            (r'(?i)\bMAIN\b', TokenType.MAIN),
            (r'(?i)\bEND_FOR\b', TokenType.END_FOR),
            (r'(?i)\bEND_IF\b', TokenType.END_IF),
            (r'(?i)\bWHILE\b', TokenType.WHILE),
            (r'(?i)\bSTEP\b', TokenType.STEP),
            (r'(?i)\bELSE\b', TokenType.ELSE),
            (r'(?i)\bELIF\b', TokenType.ELIF),
            (r'(?i)\bIF\b', TokenType.IF),
            (r'(?i)\bFOR\b', TokenType.FOR),
            (r'(?i)\bTO\b', TokenType.TO),
            (r'(?i)\bIN\b', TokenType.IN),

            # âœ… ë…¼ë¦¬ ì—°ì‚°ì
            (r'(?i)\bAND\b', TokenType.LOGICAL_OPERATOR),
            (r'(?i)\bOR\b', TokenType.LOGICAL_OPERATOR),
            (r'(?i)\bNOT\b', TokenType.LOGICAL_OPERATOR),

            # âœ… ë£¨í”„ ì œì–´
            (r'(?i)\bBREAK\b', TokenType.BREAK),
            (r'(?i)\bCONTINUE\b', TokenType.CONTINUE),

            # âœ… YmdTime í‚¤ì›Œë“œ
            (r"(?i)\bYmdTime\b", TokenType.IDENTIFIER),
            (r"(?i)\bYmd\b", TokenType.IDENTIFIER),

            # âœ… ì‘ì€ë”°ì˜´í‘œ ì˜¤ë¥˜ ê°ì§€
            (r"'([^']*)'", None),

            # âœ… ì—°ì‚°ì ë° êµ¬ë¬¸
            (r'\(', TokenType.LEFT_PAREN),
            (r'\)', TokenType.RIGHT_PAREN),
            (r'\[', TokenType.LEFT_BRACKET),
            (r'\]', TokenType.RIGHT_BRACKET),
            (r'\{', TokenType.LEFT_BRACE),
            (r'\}', TokenType.RIGHT_BRACE),
            (r',', TokenType.COMMA),
            (r':', TokenType.COLON),
            (r'==|!=|>=|<=|[+\-*/%<>]', TokenType.OPERATOR),
            (r'=', TokenType.ASSIGN),

            # âœ… ë¬¸ìì—´ (ì ‘ë‘ì–´ í¬í•¨ ë¬¸ìì—´ì€ ë”°ë¡œ ì²˜ë¦¬)
            (string_pattern, TokenType.STRING),
            # âœ… ì‹ë³„ì
            (r'[a-zA-Z_\$][a-zA-Z0-9_]*', TokenType.IDENTIFIER),

            # âœ… ìˆ«ì
            (r'\b\d+\.\d+|\.\d+|\d+\.\b', TokenType.FLOAT),
            (r'\b\d+\b', TokenType.INTEGER),

        ]

        column_num = ppLine.original_column
        line_num = ppLine.original_line

        while line:
            matched = False

            while line and line[0] == " ":
                column_num += 1
                line = line[1:]

            for pattern, token_type in token_patterns:
                match = re.match(pattern, line)
                if match:
                    if token_type is None:
                        raise KavanaSyntaxError(
                            f"ì˜ëª»ëœ ë¬¸ìì—´ í˜•ì‹ì…ë‹ˆë‹¤: ìŒë”°ì˜´í‘œë¥¼ ì‚¬ìš©í•´ ì£¼ì‹­ì‹œì˜¤ (\") ì¤„ë²ˆí˜¸ {line_num}, ì»¬ëŸ¼ë²ˆí˜¸ {column_num}"
                        )

                    if token_type == TokenType.STRING:
                        string_token = CommandParser.parse_string_token(match.group(0), line_num, column_num)
                        tokens.append(string_token)
                    else:
                        raw_value = match.group(0)
                        value = raw_value
                        value_datatype_changed = TokenUtil.primitive_to_kavana_by_tokentype(value, token_type)
                        tokens.append(Token(data=value_datatype_changed, type=token_type, line=line_num, column=column_num))

                    column_num += len(match.group(0))
                    line = line[len(match.group(0)):]
                    matched = True
                    break

            if not matched and line:
                line = line[1:]
                column_num += 1

        tokens = CommandParser.post_process_tokens(tokens)
        return tokens


    @staticmethod
    def parse_string_token(raw_string: str, line_num: int, column_num: int) -> StringToken:
        match = re.match(r'(?i)(rf|fr|r|f)?("((?:\\.|[^"\\])*)")', raw_string)

        if not match:
            raise KavanaSyntaxError(f"ì˜ëª»ëœ ë¬¸ìì—´ í˜•ì‹ì…ë‹ˆë‹¤: {raw_string}")

        prefix = (match.group(1) or "").lower()
        quoted = match.group(2)
        inner = match.group(3)

        is_raw = 'r' in prefix
        is_formatted = 'f' in prefix

        if is_raw:
            decoded = inner
        else:
            decoded = CommandParser.decode_escaped_string(inner)

        expressions = []
        if is_formatted:
            parts = re.split(r'(\{.*?\})', decoded)
            for part in parts:
                if part.startswith('{') and part.endswith('}'):
                    inner_expr = part[1:-1].strip()
                    expr_tokens = CommandParser.tokenize(PreprocessedLine(inner_expr, line_num, column_num))
                    expressions.append(expr_tokens)

        return StringToken(
            data=String(decoded),
            type=TokenType.STRING,
            line=line_num,
            column=column_num,
            is_raw=is_raw,
            is_formatted=is_formatted,
            expressions=expressions if expressions else None
        )


    # @staticmethod
    # def parse_string_token(raw_string: str, line_num: int, column_num: int) -> StringToken:
    #     import re

    #     # match = re.match(r'(?i)(r?f?|fr?)("((?:\\.|[^"\\])*)")', raw_string)
    #     match = re.match(r'(?i)(rf|fr|r|f)?("((?:\\.|[^"\\])*)")', raw_string)
    #     if not match:
    #         raise KavanaSyntaxError(f"ì˜ëª»ëœ ë¬¸ìì—´ í˜•ì‹ì…ë‹ˆë‹¤: {raw_string}")

    #     prefix = match.group(1).lower()
    #     quoted = match.group(2)
    #     inner = match.group(3)

    #     is_raw = 'r' in prefix
    #     is_formatted = 'f' in prefix

    #     if is_raw:
    #         decoded = inner
    #     else:
    #         decoded = CommandParser.decode_escaped_string(inner)

    #     expressions = []
    #     if is_formatted:
    #         parts = re.split(r'(\{.*?\})', decoded)
    #         for part in parts:
    #             if part.startswith('{') and part.endswith('}'):
    #                 inner_expr = part[1:-1].strip()
    #                 expr_tokens = CommandParser.tokenize(PreprocessedLine(inner_expr, line_num, column_num))
    #                 expressions.append(expr_tokens)

    #     return StringToken(
    #         data=decoded,
    #         type=TokenType.STRING,
    #         line=line_num,
    #         column=column_num,
    #         is_raw=is_raw,
    #         is_formatted=is_formatted,
    #         expressions=expressions if expressions else None
    #     )


    @staticmethod
    def post_process_tokens(tokens: List[Token]) -> List[Token]:
        ''' Array, AccessIndexTokenì„ ìƒì„±í•´ì„œ ëŒ€ì²´í•œë‹¤'''
        if not tokens:
            return []

        processed_tokens = []
        i = 0

        while i < len(tokens):
            token = tokens[i]

            if CommandParser._is_access_index_start(tokens, i):
                access_token, i = CommandParser.make_access_index_token(tokens, i)
                processed_tokens.append(access_token)

            elif token.type == TokenType.LEFT_BRACKET:
                array_token, i = CommandParser.make_array_token(tokens, i)
                processed_tokens.append(array_token)
            elif token.type == TokenType.LEFT_BRACE:
                hash_token, i = CommandParser.make_hash_map_token(tokens, i)
                processed_tokens.append(hash_token)
            else:
                processed_tokens.append(token)
                i += 1

        return processed_tokens

    @staticmethod
    def _is_access_index_start(tokens: List[Token], i: int) -> bool:
        return (
            i + 1 < len(tokens)
            and tokens[i].type == TokenType.IDENTIFIER
            and tokens[i + 1].type == TokenType.LEFT_BRACKET
        )

    @staticmethod
    def make_hash_map_token(tokens: List[Token], start_index: int) -> Tuple[HashMapToken, int]:
        assert tokens[start_index].type == TokenType.LEFT_BRACE

        i = start_index + 1
        end_idx = CommandParser.find_matching_brace(tokens, start_index)
        hashmap_content: Dict[Union[str, int], List[Token]] = {}

        while i < end_idx:
            key_token = tokens[i]
            if key_token.type not in [TokenType.STRING, TokenType.INTEGER]:
                raise CommandParserError("HashMapì˜ keyëŠ” ë¬¸ìì—´ ë˜ëŠ” ì •ìˆ˜ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.", key_token.line, key_token.column)

            key = key_token.data.value
            i += 1

            if i >= len(tokens) or tokens[i].type != TokenType.COLON:
                raise CommandParserError("HashMap í•­ëª©ì— ':' êµ¬ë¬¸ì´ ë¹ ì¡ŒìŠµë‹ˆë‹¤.", tokens[i].line, tokens[i].column)
            i += 1

            value_tokens = []
            while i < end_idx and tokens[i].type != TokenType.COMMA:
                if tokens[i].type == TokenType.LEFT_BRACE:
                    sub_hashmap_token, i = CommandParser.make_hash_map_token(tokens, i)
                    value_tokens.append(sub_hashmap_token)
                elif tokens[i].type == TokenType.LEFT_BRACKET:
                    sub_array_token, i = CommandParser.make_array_token(tokens, i)
                    value_tokens.append(sub_array_token)
                elif CommandParser._is_access_index_start(tokens, i):
                    access_token, i = CommandParser.make_access_index_token(tokens, i)
                    value_tokens.append(access_token)
                else:
                    value_tokens.append(tokens[i])
                    i += 1

            hashmap_content[key] = CommandParser.post_process_tokens(value_tokens)

            if i < end_idx and tokens[i].type == TokenType.COMMA:
                i += 1

        return HashMapToken(
            data=HashMap({}),
            key_express_map=hashmap_content
        ), end_idx + 1


    @staticmethod
    def find_matching_brace(tokens: List[Token], start_idx: int) -> int:
        count = 1
        i = start_idx + 1
        while i < len(tokens):
            if tokens[i].type == TokenType.LEFT_BRACE:
                count += 1 
            elif tokens[i].type == TokenType.RIGHT_BRACE:
                count -= 1
                if count == 0:
                    return i
            i += 1
        raise CommandParserError("HashMap ì¤‘ê´„í˜¸ê°€ ë‹«íˆì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", tokens[start_idx].line, tokens[start_idx].column)


    @staticmethod
    def make_access_index_token(tokens: List[Token], start_index: int) -> Tuple[AccessIndexToken, int]:
        var_name = tokens[start_index].data.value
        i = start_index + 1  # '[' ì‹œì‘ ìœ„ì¹˜
        end_idx = CommandParser.find_matching_bracket(tokens, i)
        row_sub, col_sub, pos = CommandParser.extract_row_column_expresses(tokens, i, end_idx)
        row_expr = CommandParser.post_process_tokens(row_sub)
        col_expr = CommandParser.post_process_tokens(col_sub) if col_sub else []

        return AccessIndexToken(data=String(var_name), row_express=row_expr, column_express=col_expr), pos + 1

    @staticmethod
    def make_array_token(tokens: List[Token], start_index: int) -> Tuple[ArrayToken, int]:
        list_elements = []
        current_element = []
        end_idx = CommandParser.find_matching_bracket(tokens, start_index)
        i = start_index + 1

        while i <= end_idx:
            token = tokens[i]

            if CommandParser._is_access_index_start(tokens, i):
                access_token, i = CommandParser.make_access_index_token(tokens, i)
                current_element.append(access_token)
                continue  # âœ… ì´ë¯¸ iê°€ ì´ë™ëìœ¼ë¯€ë¡œ skip

            elif token.type == TokenType.LEFT_BRACKET:
                sub_array_token, i = CommandParser.make_array_token(tokens, i)
                current_element.append(sub_array_token)
                continue  # âœ… i ì´ë¯¸ ì´ë™ë¨

            elif token.type == TokenType.COMMA:
                if current_element:
                    list_elements.append(CommandParser.post_process_tokens(current_element))
                    current_element = []
                i += 1
                continue

            elif token.type == TokenType.RIGHT_BRACKET:
                if current_element:
                    list_elements.append(CommandParser.post_process_tokens(current_element))
                break  # âœ… while íƒˆì¶œ (iëŠ” end_idx + 1ë¡œ ë°˜í™˜ë¨)

            else:
                current_element.append(token)
                i += 1  # âœ… ì¼ë°˜ í† í°ì¼ ê²½ìš°ì—ë„ ì¦ê°€

        return ArrayToken(data=Array([]), element_expresses=list_elements), end_idx + 1


    @staticmethod
    def find_matching_bracket(tokens: List[Token], start_idx: int) -> int:
        """
        ì£¼ì–´ì§„ `start_idx` ìœ„ì¹˜ì˜ `[`ì™€ ì§ì„ ì´ë£¨ëŠ” `]`ì˜ ìœ„ì¹˜ë¥¼ ì°¾ëŠ” í•¨ìˆ˜.
        '['ì—ì„œ ì‹œì‘ ']'ì˜ indexë¥¼ ë¦¬í„´í•œë‹¤.
        """
        count_bracket = 1  # `[`ë¥¼ ë§Œë‚˜ê³  ì‹œì‘í•˜ë¯€ë¡œ 1ë¡œ ì´ˆê¸°í™”
        i = start_idx + 1

        while i < len(tokens):
            if tokens[i].type == TokenType.LEFT_BRACKET:
                count_bracket += 1
            elif tokens[i].type == TokenType.RIGHT_BRACKET:
                count_bracket -= 1
                if count_bracket == 0:
                    return i  # ì§ì„ ì´ë£¨ëŠ” `]`ì˜ ìœ„ì¹˜ ë°˜í™˜
            i += 1

        raise CommandParserError("ë¦¬ìŠ¤íŠ¸ ì¸ë±ì‹±ì˜ ê´„í˜¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë‹«íˆì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", tokens[start_idx].line, tokens[start_idx].column)


    @staticmethod
    def extract_row_column_expresses(tokens: List[Token], start_idx: int, end_idx: int) -> Tuple[List[Token], List[Token], int]:
        ''' 
        ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼ í‘œí˜„ì‹ì„ íŒŒì‹±í•˜ì—¬ row_tokens, column_tokens, ë§ˆì§€ë§‰ indexë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
        
        tokens[start_idx]ëŠ” ë°˜ë“œì‹œ LEFT_BRACKET ('[') ì´ì–´ì•¼ í•˜ë©°,
        COMMA (',')ê°€ ë‚˜ì˜¤ë©´ rowì™€ columnì„ êµ¬ë¶„í•œë‹¤.
        
        `end_idx`ë¥¼ ì‚¬ìš©í•˜ì—¬ íƒìƒ‰ ë²”ìœ„ë¥¼ ì œí•œí•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •.
        '''
        
        if tokens[start_idx].type != TokenType.LEFT_BRACKET:
            raise CommandParserError("ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼ í‘œí˜„ì‹ì€ ë°˜ë“œì‹œ '['ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.", tokens[start_idx].line, tokens[start_idx].column)

        row_tokens = []
        column_tokens = []
        i = start_idx + 1  # '[' ë‹¤ìŒë¶€í„° ì‹œì‘
        count_bracket = 1  # ì²˜ìŒ '['ì„ ë§Œë‚¬ìœ¼ë¯€ë¡œ 1ë¡œ ì‹œì‘
        is_row = True  # ì²˜ìŒì—ëŠ” rowë¥¼ ì±„ì›€

        while i <= end_idx:  # ğŸ”¥ end_idxê¹Œì§€ë§Œ íƒìƒ‰í•˜ë„ë¡ ìˆ˜ì •
            token = tokens[i]

            # ê´„í˜¸ ê°œìˆ˜ ì¹´ìš´íŒ…
            if token.type == TokenType.LEFT_BRACKET:
                count_bracket += 1
            elif token.type == TokenType.RIGHT_BRACKET:
                count_bracket -= 1

            # ','ë¥¼ ë§Œë‚˜ë©´ column_tokensë¡œ ì „í™˜
            if token.type == TokenType.COMMA and count_bracket == 1:
                is_row = False
            elif token.type == TokenType.RIGHT_BRACKET and count_bracket == 0:
                break
            else:
                if is_row:
                    row_tokens.append(token)
                else:
                    column_tokens.append(token)

            i += 1

        # ê´„í˜¸ê°€ ì œëŒ€ë¡œ ë‹«íˆì§€ ì•Šì•˜ëŠ”ì§€ ê²€ì‚¬
        if count_bracket != 0:
            raise CommandParserError("ë¦¬ìŠ¤íŠ¸ ì¸ë±ì‹±ì˜ ê´„í˜¸ê°€ ì˜¬ë°”ë¥´ê²Œ ë‹«íˆì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", tokens[i].line, tokens[i].column)

        if len(row_tokens) == 0:
            raise CommandParserError("ë¦¬ìŠ¤íŠ¸ ì¸ë±ìŠ¤ì˜ ì²« ë²ˆì§¸ ê°’(row)ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", tokens[start_idx].line, tokens[start_idx].column)

        return row_tokens, column_tokens, i

